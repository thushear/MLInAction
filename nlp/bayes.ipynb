{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as py\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 粗暴的词去重\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode(\"utf-8\")\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100: # 怕内存爆掉，只取100个样本文件，你可以注释掉取完\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path, file), 'r',encoding='utf-8') as fp:\n",
    "               raw = fp.read()\n",
    "            ## 是的，随处可见的jieba中文分词\n",
    "            ##jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw, cut_all=False) # 精确模式，返回的结构是一个可迭代的genertor\n",
    "            word_list = list(word_cut) # genertor转化为list，每个词unicode格式\n",
    "            ##jieba.disable_parallel() # 关闭并行分词模式\n",
    "            \n",
    "            data_list.append(word_list) #训练集list\n",
    "            class_list.append(folder) #类别\n",
    "            j += 1\n",
    "    \n",
    "    ## 粗暴地划分训练集和测试集\n",
    "    data_class_list = zip(data_list, class_list)\n",
    "    random.shuffle(data_class_list)\n",
    "    index = int(len(data_class_list)*test_size)+1\n",
    "    train_list = data_class_list[index:]\n",
    "    test_list = data_class_list[:index]\n",
    "    train_data_list, train_class_list = zip(*train_list)\n",
    "    test_data_list, test_class_list = zip(*test_list)\n",
    "    \n",
    "    #其实可以用sklearn自带的部分做\n",
    "    #train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "\n",
    "    # 统计词频放入all_words_dict\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if all_words_dict.has_key(word):\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list\n",
    "    all_words_list = list(zip(*all_words_tuple_list)[0])\n",
    "\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3e96b992e80c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Database/SogouC/Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-cc9efac74303>\u001b[0m in \u001b[0;36mtext_processing\u001b[1;34m(folder_path, test_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m## 粗暴地划分训练集和测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdata_class_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_class_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_class_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtrain_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_class_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\soft\\anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m(self, x, random)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mrandbelow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m                 \u001b[1;31m# pick an element in x[:i+1] with which to exchange x[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "folder_path = './Database/SogouC/Sample'\n",
    "text_processing(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
